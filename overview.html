<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2018-08-12 Sun 17:33 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Human pose estimation using OpenPose</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="Sarath.M" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://www.pirilampo.org/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://www.pirilampo.org/styles/readtheorg/js/readtheorg.js"></script>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2018 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
</head>
<body>
<div id="content">
<h1 class="title">Human pose estimation using OpenPose</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org0e58f18">1. OpenPose</a>
<ul>
<li><a href="#org3144ce1">1.1. OpenPose vs Kinect</a></li>
<li><a href="#orgc5ce718">1.2. Image Segmentation</a></li>
</ul>
</li>
<li><a href="#orgbc4d4b5">2. Porting from Caffe to Tensorflow</a></li>
<li><a href="#orgf603279">3. Working of OpenPose</a>
<ul>
<li><a href="#org934c733">3.1. Parts and Pairs</a></li>
<li><a href="#orgc0a0f77">3.2. Tensorflow Implementation</a>
<ul>
<li><a href="#orgb5518db">3.2.1. Preprocessing</a></li>
<li><a href="#org6e05943">3.2.2. Neural network</a></li>
<li><a href="#orga8f7903">3.2.3. Non Maximum Suppression</a></li>
<li><a href="#org3c3f0ae">3.2.4. Bipartite graph</a></li>
<li><a href="#org6f1eb93">3.2.5. Line Integral</a></li>
<li><a href="#orgbf48b16">3.2.6. Assignment</a></li>
<li><a href="#org65971e7">3.2.7. Merging</a></li>
<li><a href="#org26ee2da">3.2.8. Output</a></li>
</ul>
</li>
<li><a href="#org4a123f0">3.3. Github Repository</a>
<ul>
<li><a href="#orgc1f1436">3.3.1. Installation</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#orgbea3e21">4. Reference</a></li>
</ul>
</div>
</div>

<div id="outline-container-org0e58f18" class="outline-2">
<h2 id="org0e58f18"><span class="section-number-2">1</span> OpenPose</h2>
<div class="outline-text-2" id="text-1">

<div class="figure">
<p><img src="./images/openpose.png" alt="openpose.png" />
</p>
</div>


<div class="figure">
<p><img src="https://cdn-images-1.medium.com/max/800/1*zNs763GXlKR2zOAbJ8j72Q.gif" alt="1*zNs763GXlKR2zOAbJ8j72Q.gif" />
</p>
</div>
</div>

<div id="outline-container-org3144ce1" class="outline-3">
<h3 id="org3144ce1"><span class="section-number-3">1.1</span> OpenPose vs Kinect</h3>
<div class="outline-text-3" id="text-1-1">
<ul class="org-ul">
<li>both Kinect and Openpose have the same net result: allowing real
time pose estimation of human bodies. The main difference is the
hardware: While Kinect uses a special <b>3D camera</b>, OpenPose works on
any <b>webcam</b></li>
<li><p>
Pose estimation from a depth image
</p>


<div class="figure">
<p><img src="./images/kinect.jpeg" alt="kinect.jpeg" />
</p>
</div>
<ul class="org-ul">
<li>In this algorithm, the first two steps are very important, and it
is here that the Kinect hardware is crucial</li>
<li>thanks to the Kinect hardware, you essentially get the first two
steps for free</li>
<li>These first two steps are essentially the biggest challenge
tackled by OpenPose and boils down to an important research domain
in Computer Vision: <b>image segmentation</b></li>
</ul></li>
</ul>
</div>
</div>

<div id="outline-container-orgc5ce718" class="outline-3">
<h3 id="orgc5ce718"><span class="section-number-3">1.2</span> Image Segmentation</h3>
<div class="outline-text-3" id="text-1-2">

<div class="figure">
<p><img src="./images/segmentation.png" alt="segmentation.png" />
</p>
</div>
<ul class="org-ul">
<li>The big difference is that, instead of a 3D human blob, OpenPose
starts out with a completely flat image, containing both the
person(s) of interest and the background. Obviously, the first part
of the OpenPose algorithm therefore needs to seperate the POI’s from
the background. This is exactly what Image Segmentation tries to
achieve.</li>
</ul>
</div>
</div>
</div>
<div id="outline-container-orgbc4d4b5" class="outline-2">
<h2 id="orgbc4d4b5"><span class="section-number-2">2</span> Porting from Caffe to Tensorflow</h2>
<div class="outline-text-2" id="text-2">
<ul class="org-ul">
<li>The OpenPose library is built upon a neural network and has been
developed by Carnegie Mellon University with astounding help of COCO
and MPII datasets</li>
<li><p>
Github repo
</p>
<div class="org-src-container">
<pre class="src src-sh">git clone https://github.com/CMU-Perceptual-Computing-Lab/openpose.git
</pre>
</div></li>
<li>OpenPose gathers three sets of trained models: one for body pose
estimation, another one for hands and a last one for faces. And each
set has several models depending on the dataset they have been
trained on (COCO or MPII)</li>
<li>So let’s begin with the body pose estimation model trained on
MPII. We need <b>two files</b>: one that describes the architecture of the
model <a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/models/pose/mpi/pose_deploy_linevec_faster_4_stages.prototxt">(.prototxt)</a> and one that stores the variables values of the
model <a href="http://posefs1.perception.cs.cmu.edu/OpenPose/models/pose/mpi/pose_iter_160000.caffemodel">(.caffemodel)</a></li>
<li><p>
To convert these files to tensorflow use the following repo:
</p>
<div class="org-src-container">
<pre class="src src-sh">git clone https://github.com/ethereon/caffe-tensorflow
</pre>
</div>
<p>
The key script here is convert.py, which takes those two files as
arguments along with paths for the new couple of files.
</p>
<div class="org-src-container">
<pre class="src src-sh">convert.py pose_deploy_linevec_faster_4_stages.prototxt <span style="color: #008000;">\</span>
--caffemodel pose_iter_160000.caffemodel --data_output_path data/output/path.npy <span style="color: #008000;">\</span>
--code_output_path code/output/path.py
</pre>
</div></li>
<li><p>
This will return a Python class that describes the architecture
(graph) of the model and an .npy file storing the values of the
TensorFlow variables.
To convert npy file to .ckpt file use
</p>
<div class="org-src-container">
<pre class="src src-sh">git clone https://github.com/alesolano/npy2ckpt
</pre>
</div></li>
<li><p>
Finally, here's the openpose code in Tensorflow
0+BEGIN_SRC python -n
  import tensorflow as tf
  import cv2
</p>

<p>
### GRAPH ###
tf.reset_default_graph()
</p>

<p>
saver = tf.train.import_meta_graph('/tmp/openpose_model.ckpt.meta')
</p>

<p>
inputs = tf.get_default_graph().get_tensor_by_name('Placeholder:0')
body = tf.get_default_graph().get_tensor_by_name('concat_stage7:0')
</p>

<p>
print('The input placeholder is expecting an array of shape {} and type {}'.format(inputs.shape, inputs.dtype))
</p>

<p>
### IMAGE ###
img = cv2.imread('chuck-norris-kick.jpg')
img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) 
res_img = cv2.resize(img, (656, 368))
prep_img = res_img.reshape([1, 368, 656, 3])
</p>

<p>
### SESSION ###
with tf.Session() as sess:
    saver.restore(sess, '/tmp/openpose_model.ckpt')
</p>

<p>
output_img = sess.run(body, feed_dict={
        inputs: prep_img
    })
</p>

<p>
  print(output_img.shape)
#+END_SRC
</p></li>
<li>And the result is an image with 57 layers of depth. Mmmm… This is not exactly what I want. Why 57 layers?
<ul class="org-ul">
<li>18 layers for body parts location</li>
<li>1 layer for the background</li>
<li>19 layers for limbs information in the x direction</li>
<li>19 layers for limbs information in the y direction</li>
</ul></li>
<li>Is there a way to combine joints and limbs? Of course. OpenPose is
not just a set of trained neural networks, it’s an entire
library. And one of the functions inside this library is
bodyPartConnectorCaffe.cpp</li>
<li><p>
I’m just researching this right now, and it seems that the
developers of OpenPose have made the work as easiest as
possible. The Caffe code is perfectly isolated and there are only a
few C++ files that depend on Caffe. These files are:
</p>
<blockquote>
<p>
core/maximumCaffe.cpp
core/nmsCaffe.cpp
core/netCaffe.cpp
core/resizeAndMergeCaffe.cpp
pose/poseExtractorCaffe.cpp
pose/bodyPartConnectorCaffe.cpp
</p>
</blockquote></li>
</ul>
</div>
</div>
<div id="outline-container-orgf603279" class="outline-2">
<h2 id="orgf603279"><span class="section-number-2">3</span> Working of OpenPose</h2>
<div class="outline-text-2" id="text-3">
<p>
<img src="./images/openPose_pipeline.png" alt="openPose_pipeline.png" />
Each part of the pipeline will be explained in the following sections
</p>
</div>
<div id="outline-container-org934c733" class="outline-3">
<h3 id="org934c733"><span class="section-number-3">3.1</span> Parts and Pairs</h3>
<div class="outline-text-3" id="text-3-1">
<ul class="org-ul">
<li>A body <b>part</b> is an element of the body, like neck, left shoulder or right hip.</li>
<li><p>
A <b>pair</b> is a couple of parts. A connection between parts. We could
say limb, but the connection between the nose and the left eye is
definitely not a limb. Also, we are going to deal with connections
between ears and shoulders that do not exist in real life.
</p>


<div class="figure">
<p><img src="./images/parts_pairs.png" alt="parts_pairs.png" />
</p>
</div></li>
</ul>
</div>
</div>
<div id="outline-container-orgc0a0f77" class="outline-3">
<h3 id="orgc0a0f77"><span class="section-number-3">3.2</span> Tensorflow Implementation</h3>
<div class="outline-text-3" id="text-3-2">
</div>
<div id="outline-container-orgb5518db" class="outline-4">
<h4 id="orgb5518db"><span class="section-number-4">3.2.1</span> Preprocessing</h4>
<div class="outline-text-4" id="text-3-2-1">
<ul class="org-ul">
<li>First but not least, we convert the image from [0,</li>
</ul>
<p>
255] to [-1, 1].
0+BEGIN_SRC python -n
img = img * (2.0 / 255.0) — 1.0
#+END_SRC
</p>
</div>
</div>
<div id="outline-container-org6e05943" class="outline-4">
<h4 id="org6e05943"><span class="section-number-4">3.2.2</span> Neural network</h4>
<div class="outline-text-4" id="text-3-2-2">
<ul class="org-ul">
<li><p>
Here we do the main processing. The last operation of the neural
network returns a tensor consisting of 57 matrices. However, this
last op is just a concatenation of two different tensors:
<b>heatmaps</b> and <b>PAF's</b>. The understanding of these two tensors is
essential.  
</p>


<div class="figure">
<p><img src="./images/hm_paf.png" alt="hm_paf.png" />
</p>
</div>
<ol class="org-ol">
<li><p>
A <b>heatmap</b> is a matrix that stores the confidence the network has
that a certain pixel contains a certain part. There are <b>18 (+1)</b>
<b>heatmaps</b> associated with each one of the parts and indexed as we
showed in the drawing of the skeletons. We will extract the
location of the body parts out of these 18 matrices.
</p>


<div class="figure">
<p><img src="./images/heatmap.png" alt="heatmap.png" />
</p>
</div></li>
<li><p>
<b>PAFs</b> (<i>Part Affinity Fields</i>) are matrices that give information
about the position and orientation of pairs. They come in
couples: for each part we have a PAF in the ‘x’ direction and a
PAF in the ‘y’ direction. There are 38 PAFs associated with each
one of the pairs and indexed as we showed in the drawing of the
skeletons. We will associate couples of parts into pairs thanks
to these 38 matrices.
</p>


<div class="figure">
<p><img src="./images/paf.png" alt="paf.png" />
</p>
</div></li>
</ol>
<p>
Code implementation:
0+BEGIN_SRC python -n
 import tensorflow as tf
</p>

<p>
inputs = tf.get_default_graph().get_tensor_by_name('inputs:0')
heatmaps_tensor = tf.get_default_graph().get_tensor_by_name('Mconv7_stage6_L2/BiasAdd:0')
pafs_tensor = tf.get_default_graph().get_tensor_by_name('Mconv7_stage6_L1/BiasAdd:0')
</p>

<p>
 with tf.Session() as sess:
     heatmaps, pafs = sess.run([heatmaps_tensor, pafs_tensor], feed_dict={
         inputs: image
 })
#+END_SRC
</p></li>
</ul>
</div>
</div>
<div id="outline-container-orga8f7903" class="outline-4">
<h4 id="orga8f7903"><span class="section-number-4">3.2.3</span> Non Maximum Suppression</h4>
<div class="outline-text-4" id="text-3-2-3">
<p>
Next step is detecting the parts in the image.
</p>

<p>
We need to extract parts locations out of a heatmap. Or, in other
words, we need to extract points out of a function. What are these
points? The local maximums.
</p>


<div class="figure">
<p><img src="./images/nms.png" alt="nms.png" />
</p>
</div>

<p>
We apply a non-maximum suppression (NMS) algorithm to get those peaks.
</p>
<ol class="org-ol">
<li>Start in the first pixel of the heatmap.</li>
<li>Surround the pixel with a window of side 5 and find the maximum value in that area.</li>
<li>Substitute the value of the center pixel for that maximum</li>
<li>Slide the window one pixel and repeat these steps after we’ve covered the entire heatmap.</li>
<li>Compare the result with the original heatmap. Those pixels staying
with same value are the peaks we are looking for. Suppress the
other pixels setting them with a value of 0</li>
</ol>

<p>
After all the process, the non-zero pixels denote the location of the
part candidates.
</p>

<ul class="org-ul">
<li>Source code</li>
</ul>
<p>
0+BEGIN_SRC python -n
from scipy.ndimage.filters import maximum filter
</p>

<p>
part_candidates = heatmap*(heatmap == maximum_filter(heatmap, footprint=np.ones((window_size, window_size))))
#+END_SRC
</p>
</div>
</div>
<div id="outline-container-org3c3f0ae" class="outline-4">
<h4 id="org3c3f0ae"><span class="section-number-4">3.2.4</span> Bipartite graph</h4>
<div class="outline-text-4" id="text-3-2-4">
<p>
Now that we have found the candidates for each one of the body parts,
we need to connect them to form pairs. And this is where graph theory
steps into.
</p>

<p>
Say that, for a given image, we have located a set of neck candidates
and a set of right hip candidates. For each neck there is a possible
association, or connection candidate, with each one of the right
hips. So, what we have, is a <a href="https://en.wikipedia.org/wiki/Complete_bipartite_graph">complete bipartite graph</a>, where the
vertices are the part candidates, and the edges are the connection
candidates.
</p>


<div class="figure">
<p><img src="./images/bipartite_graph.png" alt="bipartite_graph.png" />
</p>
</div>

<p>
How can we find the right connections? Finding the best matching
between vertices of a bipartite graph is a well-known problem in graph
theory known as the <a href="https://en.wikipedia.org/wiki/Assignment_problem">assignment problem</a>. In order to solve it, each
edge on the graph should have a weight. 
</p>
</div>
</div>
<div id="outline-container-org6f1eb93" class="outline-4">
<h4 id="org6f1eb93"><span class="section-number-4">3.2.5</span> Line Integral</h4>
<div class="outline-text-4" id="text-3-2-5">

<div class="figure">
<p><img src="./images/line_integral.png" alt="line_integral.png" />
</p>
</div>
<ul class="org-ul">
<li>The right connections can be found out by using line integral over PAF's</li>
<li><p>
<a href="https://en.wikipedia.org/wiki/Line_integral">Line Integral</a> measures the effect of a given field (in our case, the
Part Affinity Fields) along a given curve (in our case, the possible
connections between part candidates).
(Ref: <a href="https://www.youtube.com/watch?v=uXjQ8yc9Pdg">Khan Academy</a>)
</p>


<div class="figure">
<p><img src="./images/line_integral2.png" alt="line_integral2.png" />
</p>
</div></li>
<li>The line integral will give each connection a score, that will be
saved in a weighted bipartite graph and will allow us to solve the
assignment problem</li>
</ul>
<blockquote>
<p>
Note: in the code, we approximate the line integral by taking samples
of the dot product and sum them all.
</p>
</blockquote>
<p>
0+BEGIN_SRC python -n
import math
import numpy as np
</p>

<p>
dx, dy = x2 — x1, y2 — y1
normVec = math.sqrt(dx <b>* 2 + dy *</b> 2)
vx, vy = dx/normVec, dy/normVec
</p>

<p>
num_samples = 10
xs = np.arange(x1, x2, dx/num_samples).astype(np.int8)
ys = np.arange(y1, y2, dy/num_samples).astype(np.int8)
</p>

<p>
pafXs = pafX[ys, xs]
pafYs = pafY[ys, xs]
</p>

<p>
score = sum(pafXs * vx + pafYs * vy) / num_samples
#+END_SRC
</p>
</div>
</div>
<div id="outline-container-orgbf48b16" class="outline-4">
<h4 id="orgbf48b16"><span class="section-number-4">3.2.6</span> Assignment</h4>
<div class="outline-text-4" id="text-3-2-6">
<ul class="org-ul">
<li>Now, the weighted bipartite graph shows all connections between
candidates of two parts and holds score for every connection.</li>
<li>The next step is to find the connection that has maximum total
score. Steps given below:
<ol class="org-ol">
<li>Sort each possible connection by its score.</li>
<li>The connection with the highest score is indeed a final connection.</li>
<li>Move to next possible connection. If no parts of this connection
have been assigned to a final connection before, this is a final
connection.</li>
<li>Repeat the step 3 until we are done.</li>
</ol></li>
</ul>


<div class="figure">
<p><img src="./images/assignment.png" alt="assignment.png" />
</p>
</div>
<ul class="org-ul">
<li>As you see, there may be part candidates that will finally not fit into a pair.</li>
<li>Source Code
0+BEGIN_SRC python -n</li>
</ul>
<p>
connection = []
used_idx1, used_idx2 = [], []
</p>

<p>
for conn_candidate in sorted(connection_temp, key=lambda x: x['score'], reverse=True):
</p>

<p>
  if conn_candidate['idx'][0] in used_idx1 or conn_candidate['idx'][1] in used_idx2:
      continue
  connection.append(conn_candidate)
  used_idx1.append(conn_candidate['idx'][0])
  used_idx2.append(conn_candidate['idx'][1])
#+END_SRC
</p>
</div>
</div>
<div id="outline-container-org65971e7" class="outline-4">
<h4 id="org65971e7"><span class="section-number-4">3.2.7</span> Merging</h4>
<div class="outline-text-4" id="text-3-2-7">
<ul class="org-ul">
<li>final step is to transform these detected connections into the final skeletons</li>
<li>We will start with a naive assumption: at first, every connection
belongs to a different human. This way, we have the same number of
humans as connections we have detected</li>
<li><p>
Let Humans be a collection of sets <code>{H1, H2, ..., Hk}</code>. Each one of
these sets — that is, each human — contains, at first, two parts (a
pair). And let’s describe a part as a tuple of an index, a
coordinate in the <code>‘x’</code> direction and a coordinate in the <code>‘y’</code>
direction.
</p>


<div class="figure">
<p><img src="./images/merging.png" alt="merging.png" />
</p>
</div></li>
<li><p>
if humans H1 and H2 share a part index with the same coordinates,
they are sharing the same part! H1 and H2 are, therefore, the same
humans. So we merge both sets into H1 and remove H2.
</p>


<div class="figure">
<p><img src="./images/merging2.png" alt="merging2.png" />
</p>
</div></li>
<li>We continue to do this for every couple of humans until no couple share a part.</li>
</ul>

<blockquote>
<p>
Note: in the code, for some sensible reasons, we first define a human
as a set of connections, not as set of parts. After all the merging is
done, we finally describe a human as a set of parts.
</p>
</blockquote>

<p>
0+BEGIN_SRC python -n
from collections import defaultdict
import itertools
</p>

<p>
no_merge_cache = defaultdict(list)
empty_set = set()
</p>

<p>
while True:
    is_merged = False
</p>

<p>
for h1, h2 in itertools.combinations(connections_by_human.keys(), 2):
    for c1, c2 in itertools.product(connections_by_human[h1], connections_by_human[h2]):
</p>

<p>
if set(c1['partCoordsAndIdx']) &amp; set(c2['partCoordsAndIdx']) != empty_set:
    is_merged = True
</p>

<p>
connections_by_human[h1].extend(connections_by_human[h2])
connections_by_human.pop(h2) # delete human2
break
</p>

<p>
if not is_merged: # if no more mergings are possible, then break
    break
</p>

<p>
humans = [human_conns_to_human_parts(human_conns) for human_conns in connections_by_human.values()]
#+END_SRC
</p>
</div>
</div>
<div id="outline-container-org26ee2da" class="outline-4">
<h4 id="org26ee2da"><span class="section-number-4">3.2.8</span> Output</h4>
<div class="outline-text-4" id="text-3-2-8">
<p>
Finally what you get is a collection of human sets, where each human
is a set of parts, where each part contains its index, its relative
coordinates and its score.
</p>
</div>
</div>
</div>
<div id="outline-container-org4a123f0" class="outline-3">
<h3 id="org4a123f0"><span class="section-number-3">3.3</span> Github Repository</h3>
<div class="outline-text-3" id="text-3-3">
</div>
<div id="outline-container-orgc1f1436" class="outline-4">
<h4 id="orgc1f1436"><span class="section-number-4">3.3.1</span> Installation</h4>
<div class="outline-text-4" id="text-3-3-1">
</div>
<ol class="org-ol">
<li><a id="orgbc3e972"></a>Dependencies<br />
<div class="outline-text-5" id="text-3-3-1-1">
<ul class="org-ul">
<li>Python 3</li>
<li>TensorFlow</li>
<li>OpenCV</li>
<li>Numpy</li>
<li>Scipy</li>
<li>Matplotlib installed.</li>
</ul>
</div>
</li>
<li><a id="orgc15fbe7"></a>Build the code<br />
<div class="outline-text-5" id="text-3-3-1-2">
<div class="org-src-container">
<pre class="src src-sh">git clone https://gist.github.com/alesolano/b073d8ec9603246f766f9f15d002f4f4 openpose_pipeline
<span style="color: #006FE0;">cd</span> openpose_pipeline
mkdir models
<span style="color: #006FE0;">cd</span> models
wget https://www.dropbox.com/s/2dw1oz9l9hi9avg/optimized_openpose.pb
<span style="color: #006FE0;">cd</span> ..
python inference.py --imgpath /path/to/your/img
</pre>
</div>
</div>
</li>
<li><a id="org4e2105a"></a>Main files from the repo<br />
<ol class="org-ol">
<li><a id="orgcfab3ba"></a>inference.py<br />
<div class="outline-text-6" id="text-3-3-1-3-1">
<p>
0+BEGIN_SRC python -n
'''
All code is highly based on Ildoo Kim's code (<a href="https://github.com/ildoonet/tf-openpose">https://github.com/ildoonet/tf-openpose</a>)
and derived from the OpenPose Library (<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/LICENSE">https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/LICENSE</a>)
'''
</p>

<p>
import tensorflow as tf
import cv2
import numpy as np
import argparse
</p>

<p>
from common import estimate_pose, draw_humans, read_imgfile
</p>

<p>
import time
</p>

<p>
if <span class="underline"><span class="underline">name</span></span> <code>= '__main__':
    parser = argparse.ArgumentParser(description</code>'Tensorflow Openpose Inference')
    parser.add_argument('&#x2013;imgpath', type=str, default='./images/wywh.jpg')
    parser.add_argument('&#x2013;input-width', type=int, default=656)
    parser.add_argument('&#x2013;input-height', type=int, default=368)
    args = parser.parse_args()
</p>

<p>
t0 = time.time()
</p>

<p>
tf.reset_default_graph()
</p>

<p>
from tensorflow.core.framework import graph_pb2
graph_def = graph_pb2.GraphDef()
</p>

<p>
with open('models/optimized_openpose.pb', 'rb') as f:
    graph_def.ParseFromString(f.read())
tf.import_graph_def(graph_def, name='')
</p>

<p>
t1 = time.time()
print(t1 - t0)
</p>

<p>
inputs = tf.get_default_graph().get_tensor_by_name('inputs:0')
heatmaps_tensor = tf.get_default_graph().get_tensor_by_name('Mconv7_stage6_L2/BiasAdd:0')
pafs_tensor = tf.get_default_graph().get_tensor_by_name('Mconv7_stage6_L1/BiasAdd:0')
</p>

<p>
t2 = time.time()
print(t2 - t1)
</p>

<p>
image = read_imgfile(args.imgpath, args.input_width, args.input_height)
</p>

<p>
t3 = time.time()
print(t3 - t2)
</p>

<p>
with tf.Session() as sess:
    heatMat, pafMat = sess.run([heatmaps_tensor, pafs_tensor], feed_dict={
        inputs: image
    })
</p>

<p>
t4 = time.time()
print(t4 - t3)
</p>

<p>
heatMat, pafMat = heatMat[0], pafMat[0]
</p>

<p>
humans = estimate_pose(heatMat, pafMat)
</p>

<p>
image = cv2.imread(args.imgpath)
image_h, image_w = image.shape[:2]
image = draw_humans(image, humans)
</p>

<p>
scale = 480.0 / image_h
newh, neww = 480, int(scale * image_w + 0.5)
</p>

<p>
image = cv2.resize(image, (neww, newh), interpolation=cv2.INTER_AREA)
</p>

<p>
        cv2.imshow('result', image)
        t5 = time.time()
        print(t5 - t4)
        cv2.waitKey(0)
#+END_SRC
</p>
</div>
</li>
<li><a id="org38ef132"></a>common.py<br />
<div class="outline-text-6" id="text-3-3-1-3-2">
<p>
0+BEGIN_SRC python -n
  '''
  All code is highly based on Ildoo Kim's code (<a href="https://github.com/ildoonet/tf-openpose">https://github.com/ildoonet/tf-openpose</a>)
  and derived from the OpenPose Library (<a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/LICENSE">https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/LICENSE</a>)
  '''
</p>

<p>
from collections import defaultdict
from enum import Enum
import math
</p>

<p>
import numpy as np
import itertools
import cv2
from scipy.ndimage.filters import maximum_filter
</p>


<p>
class CocoPart(Enum):
    Nose = 0
    Neck = 1
    RShoulder = 2
    RElbow = 3
    RWrist = 4
    LShoulder = 5
    LElbow = 6
    LWrist = 7
    RHip = 8
    RKnee = 9
    RAnkle = 10
    LHip = 11
    LKnee = 12
    LAnkle = 13
    REye = 14
    LEye = 15
    REar = 16
    LEar = 17
    Background = 18
</p>

<p>
CocoPairs = [
    (1, 2), (1, 5), (2, 3), (3, 4), (5, 6), (6, 7), (1, 8), (8, 9), (9, 10), (1, 11),
    (11, 12), (12, 13), (1, 0), (0, 14), (14, 16), (0, 15), (15, 17), (2, 16), (5, 17)
]   # = 19
CocoPairsRender = CocoPairs[:-2]
CocoPairsNetwork = [
    (12, 13), (20, 21), (14, 15), (16, 17), (22, 23), (24, 25), (0, 1), (2, 3), (4, 5),
    (6, 7), (8, 9), (10, 11), (28, 29), (30, 31), (34, 35), (32, 33), (36, 37), (18, 19), (26, 27)
 ]  # = 19
</p>

<p>
CocoColors = [[255, 0, 0], [255, 85, 0], [255, 170, 0], [255, 255, 0], [170, 255, 0], [85, 255, 0], [0, 255, 0],
              [0, 255, 85], [0, 255, 170], [0, 255, 255], [0, 170, 255], [0, 85, 255], [0, 0, 255], [85, 0, 255],
              [170, 0, 255], [255, 0, 255], [255, 0, 170], [255, 0, 85]]
</p>


<p>
NMS_Threshold = 0.1
InterMinAbove_Threshold = 6
Inter_Threashold = 0.1
Min_Subset_Cnt = 4
Min_Subset_Score = 0.8
Max_Human = 96
</p>


<p>
def human_conns_to_human_parts(human_conns, heatMat):
    human_parts = defaultdict(lambda: None)
    for conn in human_conns:
        human_parts[conn['partIdx'][0]] = (
            conn['partIdx'][0], # part index
            (conn['coord_p1'][0] / heatMat.shape[2], conn['coord_p1'][1] / heatMat.shape[1]), # relative coordinates
            heatMat[conn['partIdx'][0], conn['coord_p1'][1], conn['coord_p1'][0]] # score
            )
        human_parts[conn['partIdx'][1]] = (
            conn['partIdx'][1],
            (conn['coord_p2'][0] / heatMat.shape[2], conn['coord_p2'][1] / heatMat.shape[1]),
            heatMat[conn['partIdx'][1], conn['coord_p2'][1], conn['coord_p2'][0]]
            )
    return human_parts
</p>


<p>
def non_max_suppression(heatmap, window_size=3, threshold=NMS_Threshold):
    heatmap[heatmap &lt; threshold] = 0 # set low values to 0
    part_candidates = heatmap*(heatmap == maximum_filter(heatmap, footprint=np.ones((window_size, window_size))))
    return part_candidates
</p>


<p>
def estimate_pose(heatMat, pafMat):
    if heatMat.shape[2] == 19:
</p>

<p>
    heatMat = np.rollaxis(heatMat, 2, 0)
if pafMat.shape[2] == 38:
</p>

<p>
pafMat = np.rollaxis(pafMat, 2, 0)
</p>

<p>
heatMat = heatMat - heatMat.min(axis=1).min(axis=1).reshape(19, 1, 1)
heatMat = heatMat - heatMat.min(axis=2).reshape(19, heatMat.shape[1], 1)
</p>

<p>
_NMS_Threshold = max(np.average(heatMat) * 4.0, NMS_Threshold)
_NMS_Threshold = min(_NMS_Threshold, 0.3)
</p>

<p>
coords = [] # for each part index, it stores coordinates of candidates
for heatmap in heatMat[:-1]: # remove background
    part_candidates = non_max_suppression(heatmap, 5, _NMS_Threshold)
    coords.append(np.where(part_candidates &gt;= _NMS_Threshold))
</p>

<p>
connection_all = [] # all connections detected. no information about what humans they belong to
for (idx1, idx2), (paf_x_idx, paf_y_idx) in zip(CocoPairs, CocoPairsNetwork):
    connection = estimate_pose_pair(coords, idx1, idx2, pafMat[paf_x_idx], pafMat[paf_y_idx])
    connection_all.extend(connection)
</p>

<p>
conns_by_human = dict()
for idx, c in enumerate(connection_all):
    conns_by_human['human_%d' % idx] = [c] # at first, all connections belong to different humans
</p>

<p>
no_merge_cache = defaultdict(list)
empty_set = set()
while True:
    is_merged = False
    for h1, h2 in itertools.combinations(conns_by_human.keys(), 2):
        if h1 == h2:
            continue
        if h2 in no_merge_cache[h1]:
            continue
        for c1, c2 in itertools.product(conns_by_human[h1], conns_by_human[h2]):
</p>

<p>
if set(c1['uPartIdx']) &amp; set(c2['uPartIdx']) != empty_set:
    is_merged = True
</p>

<p>
        conns_by_human[h1].extend(conns_by_human[h2])
        conns_by_human.pop(h2) # delete human2
        break
if is_merged:
    no_merge_cache.pop(h1, None)
    break
else:
    no_merge_cache[h1].append(h2)
</p>

<p>
if not is_merged: # if no more mergings are possible, then break
    break
</p>

<p>
conns_by_human = {h: conns for (h, conns) in conns_by_human.items() if len(conns) &gt;= Min_Subset_Cnt}
</p>

<p>
conns_by_human = {h: conns for (h, conns) in conns_by_human.items() if max([conn['score'] for conn in conns]) &gt;= Min_Subset_Score}
</p>

<p>
humans = [human_conns_to_human_parts(human_conns, heatMat) for human_conns in conns_by_human.values()]
return humans
</p>


<p>
def estimate_pose_pair(coords, partIdx1, partIdx2, pafMatX, pafMatY):
    connection_temp = [] # all possible connections
    peak_coord1, peak_coord2 = coords[partIdx1], coords[partIdx2]
</p>

<p>
for idx1, (y1, x1) in enumerate(zip(peak_coord1[0], peak_coord1[1])):
    for idx2, (y2, x2) in enumerate(zip(peak_coord2[0], peak_coord2[1])):
        score, count = get_score(x1, y1, x2, y2, pafMatX, pafMatY)
        if (partIdx1, partIdx2) in [(2, 3), (3, 4), (5, 6), (6, 7)]: # arms
            if count &lt; InterMinAbove_Threshold // 2 or score &lt;= 0.0:
                continue
        elif count &lt; InterMinAbove_Threshold or score &lt;= 0.0:
            continue
        connection_temp.append({
            'score': score,
            'coord_p1': (x1, y1),
            'coord_p2': (x2, y2),
            'idx': (idx1, idx2), # connection candidate identifier
            'partIdx': (partIdx1, partIdx2),
            'uPartIdx': ('{}-{}-{}'.format(x1, y1, partIdx1), '{}-{}-{}'.format(x2, y2, partIdx2))
        })
</p>

<p>
connection = []
used_idx1, used_idx2 = [], []
</p>

<p>
for conn_candidate in sorted(connection_temp, key=lambda x: x['score'], reverse=True):
</p>

<p>
if conn_candidate['idx'][0] in used_idx1 or conn_candidate['idx'][1] in used_idx2:
    continue
connection.append(conn_candidate)
used_idx1.append(conn_candidate['idx'][0])
used_idx2.append(conn_candidate['idx'][1])
</p>

<p>
return connection
</p>


<p>
def get_score(x1, y1, x2, y2, pafMatX, pafMatY):
    num_inter = 10
    dx, dy = x2 - x1, y2 - y1
    normVec = math.sqrt(dx <b>* 2 + dy *</b> 2)
</p>

<p>
if normVec &lt; 1e-4:
    return 0.0, 0
</p>

<p>
vx, vy = dx / normVec, dy / normVec
</p>

<p>
xs = np.arange(x1, x2, dx / num_inter) if x1 != x2 else np.full((num_inter, ), x1)
ys = np.arange(y1, y2, dy / num_inter) if y1 != y2 else np.full((num_inter, ), y1)
xs = (xs + 0.5).astype(np.int8)
ys = (ys + 0.5).astype(np.int8)
</p>

<p>
pafXs = np.zeros(num_inter)
pafYs = np.zeros(num_inter)
for idx, (mx, my) in enumerate(zip(xs, ys)):
    pafXs[idx] = pafMatX[my][mx]
    pafYs[idx] = pafMatY[my][mx]
</p>

<p>
local_scores = pafXs * vx + pafYs * vy
thidxs = local_scores &gt; Inter_Threashold
</p>

<p>
return sum(local_scores * thidxs), sum(thidxs)
</p>


<p>
def read_imgfile(path, width, height):
    img = cv2.imread(path)
    val_img = preprocess(img, width, height)
    return val_img
</p>


<p>
def preprocess(img, width, height):
    val_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # cv2 reads in BGR format
    val_img = cv2.resize(val_img, (width, height)) # each net accept only a certain size
    val_img = val_img.reshape([1, height, width, 3])
    val_img = val_img.astype(float)
    val_img = val_img * (2.0 / 255.0) - 1.0 # image range from -1 to +1
    return val_img
</p>


<p>
def draw_humans(img, human_list):
    img_copied = np.copy(img)
    image_h, image_w = img_copied.shape[:2]
    centers = {}
    for human in human_list:
        part_idxs = human.keys()
</p>

<p>
for i in range(CocoPart.Background.value):
    if i not in part_idxs:
        continue
    part_coord = human[i][1]
    center = (int(part_coord[0] * image_w + 0.5), int(part_coord[1] * image_h + 0.5))
    centers[i] = center
    cv2.circle(img_copied, center, 3, CocoColors[i], thickness=3, lineType=8, shift=0)
</p>

<p>
for pair_order, pair in enumerate(CocoPairsRender):
    if pair[0] not in part_idxs or pair[1] not in part_idxs:
        continue
</p>

<p>
img_copied = cv2.line(img_copied, centers[pair[0]], centers[pair[1]], CocoColors[pair_order], 3)
</p>

<p>
      return img_copied
#+END_SRC
</p>
</div>
</li>
</ol>
</li>
</ol>
</div>
</div>
</div>
<div id="outline-container-orgbea3e21" class="outline-2">
<h2 id="orgbea3e21"><span class="section-number-2">4</span> Reference</h2>
<div class="outline-text-2" id="text-4">
<ol class="org-ol">
<li><a href="https://arvrjourney.com/human-pose-estimation-using-openpose-with-tensorflow-part-2-e78ab9104fc8">Human pose estimation using OpenPose with TensorFlow (Part 2)</a></li>
<li><a href="https://medium.com/brainjar/robust-real-time-pose-estimation-with-openpose-89ae39ee05ed">Robust Real-Time Pose Estimation with OpenPose</a></li>
<li><a href="https://github.com/CMU-Perceptual-Computing-Lab/openpose">OpenPose official github repo</a></li>
<li><a href="https://arxiv.org/pdf/1602.00134.pdf">Convolutional Pose Machines</a></li>
<li><a href="https://arxiv.org/pdf/1611.08050.pdf">Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields</a></li>
</ol>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="date">Date: 2018-08-12 Sun 00:00</p>
<p class="author">Author: Sarath.M</p>
<p class="date">Created: 2018-08-12 Sun 17:33</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
